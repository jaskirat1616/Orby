# Orby

The local-first AI CLI with agentic powers. Connect Ollama, LM Studio, Hugging Face â€” no cloud required.

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)
[![Local AI](https://img.shields.io/badge/local-AI-green.svg)](https://ollama.ai/)

## ğŸš€ Features

### ğŸ”¥ Enhanced CLI with Premium UI
- **Gemini/Claude-style Interface**: Rich panels, colors, and professional typography
- **TUI Mode**: Text-based user interface with panels like htop
- **Web Dashboard**: Streamlit-like web interface for visual monitoring
- **Session Management**: Save, load, and manage conversation sessions

### ğŸ¤– Advanced Agentic Capabilities
- **Built-in Tools**: Shell commands, web search, file system access, code execution
- **Plugin System**: Extend functionality with custom tools
- **Auto Mode**: Let Orby chain multiple reasoning steps autonomously
- **Memory Layer**: Ephemeral (session) + persistent (project-level) memory

### ğŸ§  Model Management
- **Multi-backend Support**: Ollama, LM Studio, vLLM, ExLlama, GPT4All
- **Auto-detection**: Automatically discovers locally installed models
- **Dynamic Switching**: `orby use mistral` or `orby use llama3-70b`
- **Benchmarking**: Rank models by performance with `orby benchmark`

### ğŸ’» Developer Experience Superpowers
- **Repo-aware Coding**: Multi-file context awareness
- **Refactoring & Debugging**: Claude/Gemini-style coding abilities
- **Live Mode**: Background monitoring with proactive suggestions
- **Context Control**: Embeddings, summaries, and RAG from local docs

### ğŸŒ Modern AI Integrations
- **Built-in RAG**: From local files (PDF, Markdown, code)
- **Voice Mode**: Local speech-to-text with Whisper
- **Vision Mode**: Image processing with LLaVA or similar
- **Streaming Outputs**: Token-level updates with timing metrics

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/orby/orby.git
cd orby

# Install with all optional dependencies
pip install -e ".[voice,vision,web]"

# Or install minimal version
pip install -e .
```

## ğŸš€ Quick Start

### 1. Start a Chat Session
```bash
orby chat
```

### 2. List Available Models
```bash
orby models
```

### 3. Benchmark Your Models
```bash
orby benchmark
```

### 4. Search Your Codebase
```bash
orby search "function that processes images"
```

### 5. Launch TUI Mode
```bash
orby tui
```

## ğŸ› ï¸ Commands

### Core Commands
```bash
orby chat [--model MODEL] [--auto] [--tui] [--session SESSION]
orby run "Your prompt here" [--model MODEL]
orby models
orby config [show|set|create-profile|use-profile]
```

### Enhanced Commands
```bash
orby use MODEL_NAME           # Switch to a different model
orby benchmark                # Benchmark available models
orby search QUERY             # Search for context in your project
orby tui                      # Launch TUI mode
orby live                     # Start live monitoring mode
orby plugins                  # List available plugins
orby install-plugin FILE      # Install a custom plugin
orby memory                   # Show memory statistics
orby clear-memory             # Clear all memory
orby context                  # Show current context
```

## ğŸ¯ Advanced Features

### Model Management
```bash
# Auto-detect and switch between models
orby models
orby use llama3.70b:latest
orby benchmark
```

### Session Management
```bash
# In chat mode:
/save session_name
/load session_name
/reset
/auto  # Toggle auto-approve mode
```

### Live Mode
```bash
orby live  # Monitor file changes and get proactive suggestions
```

### Plugin System
```bash
orby plugins
orby install-plugin my_custom_tool.py
```

## ğŸ§ª Example Usage

### Interactive Chat with Tools
```
$ orby chat
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ORBY                                                                         â”‚
â”‚ Local-first AI CLI with agentic powers                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Connected: ollama | Model: llama3.1:latest                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

You > Show me the files in the current directory
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ You â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ > Show me the files in the current directory â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â ‹ Processing...
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Orby â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ¦                                                                            â”‚
â”‚ I can help you list files in the current directory. Would you like me to   â”‚
â”‚ run 'ls -la'?                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Code Assistance
```
$ orby run "Refactor this Python function to use list comprehension"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ You â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ > Refactor this Python function to use list comprehension                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â ‹ Processing...
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Orby â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ¦                                                                            â”‚
â”‚ I can help you refactor Python code. Please provide the function you'd      â”‚
â”‚ like me to refactor, and I'll convert it to use list comprehensions.         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Benchmarking
```
$ orby benchmark
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Benchmark Results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Rank  Model                    Backend     Score                                     â”‚
â”‚ 1     llama3.1:latest        ollama      95.2                                      â”‚
â”‚ 2     mistral:latest         ollama      92.8                                      â”‚
â”‚ 3     gemma2:latest          ollama      88.5                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§° Configuration

### Default Configuration
```yaml
# ~/.orby/config.yml
default_model: llama3.1:latest
default_backend: ollama
ollama_url: http://localhost:11434
lmstudio_url: http://localhost:1234
```

### Profiles
```bash
orby config create-profile development
orby config create-profile production
orby config use-profile development
```

## ğŸ”§ Supported Backends

- **Ollama**: Local model serving with automatic model management
- **LM Studio**: Desktop app with API-compatible interface
- **vLLM**: High-throughput serving engine
- **ExLlama**: Optimized inference for GGUF models
- **GPT4All**: Cross-platform local models

## ğŸ“š Documentation

### Tools
- `shell`: Execute secure shell commands
- `code`: Execute code snippets in sandboxed environments
- `file`: Read, write, and manipulate files
- `web`: Fetch web content and perform searches
- `vision`: Process images with local vision models
- `voice`: Transcribe audio with local speech recognition

### Memory System
- **Session Memory**: Ephemeral conversation history
- **Persistent Memory**: Project-level knowledge retention
- **Context Aware**: Automatic retrieval of relevant information

### Plugin Architecture
```python
# Example custom plugin
from orby.plugins import ToolPlugin

class MyCustomTool(ToolPlugin):
    @property
    def name(self) -> str:
        return "my_tool"
    
    @property
    def description(self) -> str:
        return "My custom tool that does amazing things"
    
    async def execute(self, **kwargs) -> Dict[str, Any]:
        # Your custom logic here
        return {"status": "success", "result": "Custom tool executed"}
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a pull request

### Development Setup
```bash
git clone https://github.com/orby/orby.git
cd orby
pip install -e ".[dev]"
```

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- Inspired by Gemini CLI and Anthropic Claude
- Built with â¤ï¸ for the local AI community
- Special thanks to the Ollama, LM Studio, and Hugging Face teams

---

**Orby** - The local-first AI CLI that puts agentic power in your terminal.